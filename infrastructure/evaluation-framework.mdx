---
title: "Evaluation Framework"
description: "How HANA evaluates conversation quality, clinical accuracy, and patient experience using automated and human-in-the-loop assessment."
---

## Why Evaluation Is Hard in Voice AI

Evaluating a text-based AI output is straightforward — a clinician reads the note and checks it. Evaluating a voice-based patient conversation is fundamentally different. The output is a 5-15 minute dialogue with branching paths, emotional dynamics, and clinical data extraction happening simultaneously. No single metric captures quality.

HANA's evaluation framework assesses conversations across multiple dimensions, using a combination of automated LLM-based judging, rule-based validation, and human expert review.

## Evaluation Architecture

```
┌────────────────────────────────────────────────────────────────┐
│                    EVALUATION PIPELINE                          │
│                                                                 │
│  Completed Conversation                                         │
│         │                                                       │
│         ├──▶ ┌─────────────────────┐                           │
│         │    │ Automated Evaluation │                           │
│         │    │ (LLM-as-Judge)       │                           │
│         │    │                      │                           │
│         │    │ • Clinical accuracy  │                           │
│         │    │ • Protocol adherence │                           │
│         │    │ • Communication      │                           │
│         │    │ • Completeness       │                           │
│         │    └──────────┬──────────┘                           │
│         │               │                                       │
│         ├──▶ ┌─────────────────────┐                           │
│         │    │ Rule-Based Checks    │                           │
│         │    │                      │                           │
│         │    │ • Required fields    │                           │
│         │    │ • Timing compliance  │                           │
│         │    │ • Safety triggers    │                           │
│         │    │ • Data extraction    │                           │
│         │    └──────────┬──────────┘                           │
│         │               │                                       │
│         └──▶ ┌─────────────────────┐                           │
│              │ Human Expert Review  │                           │
│              │ (sampled)            │                           │
│              │                      │                           │
│              │ • Clinical judgment  │                           │
│              │ • Edge cases         │                           │
│              │ • Calibration anchor │                           │
│              └──────────┬──────────┘                           │
│                         │                                       │
│                         ▼                                       │
│              ┌─────────────────────┐                           │
│              │ Composite Quality    │                           │
│              │ Score (1-5)          │                           │
│              └─────────────────────┘                           │
│                                                                 │
└────────────────────────────────────────────────────────────────┘
```

## Automated Evaluation (LLM-as-Judge)

HANA uses a separate evaluation model — distinct from the conversation models — to assess completed conversations. This separation prevents the system from grading its own homework.

### Evaluation Dimensions

Each conversation is scored across four primary dimensions:

**1. Clinical Accuracy (weight: 35%)**
- Were all extracted clinical data points factually correct?
- Did the system correctly interpret patient responses?
- Were medication names, dosages, and conditions accurately captured?
- Did the system avoid fabricating or assuming clinical information?

**2. Protocol Adherence (weight: 25%)**
- Did the conversation follow the prescribed clinical protocol?
- Were all required questions asked?
- Were branching decisions made correctly based on patient responses?
- Did the system properly handle escalation triggers?

**3. Communication Quality (weight: 25%)**
- Was the language clear and appropriate for the patient?
- Did the system demonstrate empathy when appropriate?
- Was pacing natural and responsive to the patient's communication style?
- Were medical terms explained when needed?

**4. Completeness (weight: 15%)**
- What percentage of information objectives were achieved?
- Were any critical data gaps left unaddressed?
- Did the system appropriately flag items it could not complete?
- Was the conversation efficient relative to its objectives?

### Reference-Free Evaluation

A key challenge in evaluating conversations is the absence of a "correct answer." Unlike medical QA benchmarks, there is no single right way to conduct a patient intake call. HANA's evaluation system handles this through reference-free assessment:

- The judge model evaluates conversations against the **protocol specification** (what should have been accomplished) rather than a golden transcript
- Quality is measured by **goal achievement** and **constraint satisfaction** rather than transcript similarity
- The system evaluates **what was said** relative to **what was known** (EHR data + patient responses) to detect errors
- Communication quality is assessed based on established clinical communication frameworks (teach-back method, motivational interviewing principles)

### Evaluation Dataset

HANA maintains a growing evaluation corpus:

- **Production conversations**: Every conversation is automatically evaluated
- **Curated test sets**: Manually reviewed conversations covering common scenarios, edge cases, and known failure modes
- **Synthetic test cases**: Generated scenarios for systematic testing of specific capabilities
- **Adversarial examples**: Deliberately challenging conversations designed to probe system weaknesses

Current evaluation dataset: 50,000+ scored conversations across 12 clinical protocols.

## Rule-Based Validation

Alongside LLM-based evaluation, deterministic checks validate concrete requirements:

### Data Extraction Validation
```
For each conversation:
  ✓ All required fields extracted (name, DOB, conditions, medications...)
  ✓ Extracted values match expected formats (dates, phone numbers, ICD codes)
  ✓ No duplicate or contradictory extractions
  ✓ Extraction confidence scores above threshold
```

### Timing and Efficiency
```
  ✓ Conversation completed within target duration (±20%)
  ✓ No excessive pauses (> 10 seconds of system silence)
  ✓ Response latency within SLA (< 1.5 seconds p95)
  ✓ No unnecessary repetition (same question asked > 2 times)
```

### Safety Compliance
```
  ✓ All safety trigger words detected and handled
  ✓ Escalation protocols followed when triggered
  ✓ PHI handling compliant (no unnecessary disclosure)
  ✓ Consent language delivered when required
```

## Human Expert Review

Automated evaluation is calibrated and validated through ongoing human review.

### Review Process

- **Sampling strategy**: 5% of all conversations reviewed by clinical quality team
- **Stratified sampling**: Higher review rates for new protocols, flagged conversations, and edge cases
- **Dual review**: Critical conversations reviewed by two independent reviewers
- **Calibration sessions**: Monthly sessions where human reviewers and automated scores are compared and aligned

### Reviewer Qualifications

HANA's human review team includes:
- Licensed clinical professionals (RNs, LCSWs, MDs)
- Quality assurance specialists with healthcare experience
- Patient experience professionals

### Human-AI Agreement Tracking

| Dimension | Human-AI Agreement Rate | Target |
|-----------|------------------------|--------|
| Clinical accuracy | 94% | > 90% |
| Protocol adherence | 96% | > 95% |
| Communication quality | 88% | > 85% |
| Completeness | 97% | > 95% |
| Overall quality score (±0.5) | 91% | > 90% |

When agreement drops below threshold, the automated evaluation model is retrained using the human-labeled data.

## Quality Metrics and KPIs

### Primary Quality Indicators

| Metric | Description | Current | Target |
|--------|-------------|---------|--------|
| Mean quality score | Average composite score (1-5) | 4.3 | > 4.0 |
| Clinical accuracy rate | Conversations with zero clinical errors | 98.7% | > 98% |
| Protocol completion rate | Conversations that achieved all required objectives | 89% | > 85% |
| Patient satisfaction (CSAT) | Post-call patient ratings | 4.6/5 | > 4.5/5 |
| Escalation appropriateness | Correct escalation decisions | 97% | > 95% |

### Quality Trend Analysis

HANA tracks quality metrics over time to ensure continuous improvement:

- **Per-protocol trends**: Quality scores tracked per clinical protocol to identify underperforming workflows
- **Per-partner trends**: Quality tracked per healthcare organization to detect deployment-specific issues
- **Model version comparison**: A/B quality metrics across model updates to prevent regressions
- **Temporal patterns**: Quality analysis by time of day, day of week to identify operational factors

## Evaluation-Driven Improvement

Evaluation is not just measurement — it drives the improvement cycle:

1. **Low-scoring conversations** are routed to the clinical team for root cause analysis
2. **Recurring error patterns** trigger protocol redesign or model fine-tuning
3. **High-scoring conversations** are added to the training dataset as positive examples
4. **Edge cases** are extracted and added to the regression test suite
5. **Quality gate**: No model update ships to production without matching or exceeding the previous version's evaluation scores across all dimensions

## Cost and Efficiency

HANA's evaluation framework is designed for production scale:

- **Automated evaluation cost**: < $0.02 per conversation (LLM-as-judge inference)
- **Rule-based validation**: Negligible compute cost (runs in < 100ms)
- **Human review**: Targeted sampling keeps costs manageable while maintaining calibration
- **Total evaluation overhead**: < 3% of per-conversation infrastructure cost
